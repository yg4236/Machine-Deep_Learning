{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "review감정분류.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gQfmunGcrxQ",
        "outputId": "b4b4b267-b49a-4970-f605-7baa5772faf0"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchtext import data, datasets\r\n",
        "import random\r\n",
        "\r\n",
        "SEED = 5\r\n",
        "random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "\r\n",
        "# 하이퍼파라미터\r\n",
        "BATCH_SIZE = 64\r\n",
        "lr = 0.001\r\n",
        "EPOCHS = 5\r\n",
        "\r\n",
        "USE_CUDA = torch.cuda.is_available()\r\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\r\n",
        "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)\r\n",
        "\r\n",
        "# torchtext로 전처리\r\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\r\n",
        "LABEL = data.Field(sequential=False, batch_first=True)\r\n",
        "\r\n",
        "# 전체 데이터를 훈련 데이터와 테스트 데이터를 나누기\r\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)#비율?\r\n",
        "\r\n",
        "# 단어 집합\r\n",
        "TEXT.build_vocab(trainset, min_freq=5) # 단어 집합 생성\r\n",
        "LABEL.build_vocab(trainset)\r\n",
        "vocab_size = len(TEXT.vocab)\r\n",
        "n_classes = 2\r\n",
        "\r\n",
        "# 데이터 로더\r\n",
        "trainset, valset = trainset.split(split_ratio=0.8)\r\n",
        "\r\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\r\n",
        "        (trainset, valset, testset), batch_size=BATCH_SIZE,\r\n",
        "        shuffle=True, repeat=False)\r\n",
        "\r\n",
        "#print(TEXT.vocab.stoi)\r\n",
        "class GRU(nn.Module):\r\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\r\n",
        "        super(GRU, self).__init__()\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\r\n",
        "        self.dropout = nn.Dropout(dropout_p)\r\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\r\n",
        "                          num_layers=self.n_layers,\r\n",
        "                          batch_first=True)\r\n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.embed(x)\r\n",
        "        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화\r\n",
        "        x, _ = self.gru(x, h_0)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\r\n",
        "        h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\r\n",
        "        self.dropout(h_t)\r\n",
        "        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\r\n",
        "        return logit\r\n",
        "\r\n",
        "    def _init_state(self, batch_size=1):\r\n",
        "        weight = next(self.parameters()).data\r\n",
        "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\r\n",
        "model = GRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "#훈련\r\n",
        "def train(model, optimizer, train_iter):\r\n",
        "    model.train()\r\n",
        "    for b, batch in enumerate(train_iter):\r\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\r\n",
        "        y.data.sub_(1)  # 레이블 값을 0과 1로 변환\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        logit = model(x)\r\n",
        "        loss = F.cross_entropy(logit, y)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "#평가\r\n",
        "def evaluate(model, val_iter):\r\n",
        "    \"\"\"evaluate model\"\"\"\r\n",
        "    model.eval()\r\n",
        "    corrects, total_loss = 0, 0\r\n",
        "    for batch in val_iter:\r\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\r\n",
        "        y.data.sub_(1) # 레이블 값을 0과 1로 변환 \r\n",
        "\r\n",
        "        logit = model(x)\r\n",
        "        loss = F.cross_entropy(logit, y, reduction='sum')\r\n",
        "        total_loss += loss.item()\r\n",
        "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\r\n",
        "    size = len(val_iter.dataset)\r\n",
        "    avg_loss = total_loss / size\r\n",
        "    avg_accuracy = 100.0 * corrects / size\r\n",
        "    return avg_loss, avg_accuracy\r\n",
        "\r\n",
        "best_val_loss = None\r\n",
        "# for e in range(1, EPOCHS+1):\r\n",
        "#     train(model, optimizer, train_iter)\r\n",
        "#     val_loss, val_accuracy = evaluate(model, val_iter)\r\n",
        "\r\n",
        "#     print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\r\n",
        "\r\n",
        "#     # 검증 오차가 가장 적은 최적의 모델을 저장\r\n",
        "#     if not best_val_loss or val_loss < best_val_loss:\r\n",
        "#         if not os.path.isdir(\"snapshot\"):\r\n",
        "#             os.makedirs(\"snapshot\")\r\n",
        "#         torch.save(model.state_dict(), './snapshot/txtclassification.pt')\r\n",
        "#         best_val_loss = val_loss\r\n",
        "\r\n",
        "model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\r\n",
        "\r\n",
        "input_str =\"this movie is good\" # -> 1\r\n",
        "input =torch.tensor([[10, 20, 7, 56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\r\n",
        "\r\n",
        "# input_str =\"this movie is not good\" # -> 0\r\n",
        "# iput =torch.tensor([[10, 20, 7,23, 56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\r\n",
        "\r\n",
        "# input_str =\"I think the director is very smart\" #-> 1\r\n",
        "# input =torch.tensor([[9, 98, 2, 201, 7, 49, 6347, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\r\n",
        "\r\n",
        "# input_str =\"It was nice that the actors were good at acting, and I didn’t know that time was passing because of the immersion\"\r\n",
        "# input =torch.tensor([[12, 14, 314, 11, 2, 183, 66, 56, 29, 590, 4, 9, 147, 118, 11, 84, 14, 2793, 77, 5, 2, 30928, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\r\n",
        "\r\n",
        "#input_str = \"It was fun at first, but as time went on, it got boring\" #-> 0\r\n",
        "#input =torch.tensor([[12, 14, 300, 29, 1264, 18, 15, 84, 377, 870, 12, 176, 512, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\r\n",
        "answer = model(input)\r\n",
        "y = torch.tensor([1],device='cuda:0')\r\n",
        "print(\"review : %s\"%input_str)\r\n",
        "if(answer.max(1)[1].view(y.size()).data==1): print(\"-> Positive\")\r\n",
        "else: print(\"-> Negative\")\r\n",
        "#answer.max(1)[1].view(y.size()).data #의미를 이해하지는 못함\r\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu와 cuda 중 다음 기기로 학습함: cuda\n",
            "review : this movie is good\n",
            "-> Positive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}